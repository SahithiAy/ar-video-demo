<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/>
  <title>Holo-Twin QR AR Demo</title>
  <style>
    html,body { height:100%; margin:0; overflow:hidden; background:#000; }
    #container { position:fixed; inset:0; }
    /* keep the camera video hidden (we use it as texture) */
    #camVideo { display:none; }
    /* debug: small canvas showing what we feed to jsQR */
    #qrDebug { position:fixed; right:8px; bottom:8px; width:160px; height:auto; border:2px solid rgba(255,255,255,0.12); z-index:999; }
    #message { position:fixed; left:12px; top:12px; z-index:999; padding:8px 12px; background:rgba(0,0,0,0.4); color:#fff; border-radius:6px; font-family:monospace; font-size:13px; }
    #controls { position:fixed; left:12px; bottom:12px; z-index:999; }
    button { font-size:14px; padding:8px 10px; }
  </style>
</head>
<body>
  <div id="container"></div>

  <video id="camVideo" autoplay playsinline muted></video>
  <video id="overlayVideo" autoplay playsinline muted loop crossorigin="anonymous" style="display:none"></video>
  <canvas id="qrDebug"></canvas>

  <div id="message">Loading…</div>
  <div id="controls"><button id="btnStart">Start AR</button></div>

  <!-- Libraries (CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.153.0/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jsqr/dist/jsQR.js"></script>

  <script>
  // ---------- CONFIG ----------
  const QR_SCAN_INTERVAL = 120;  // ms between QR decode attempts (tune for perf)
  const DETECT_WIDTH = 480;      // width of the small canvas we decode from (keeps CPU low)
  // Chroma key parameters (tune if needed)
  const KEY_COLOR = { r:0.0, g:1.0, b:0.0 }; // pure green reference
  const SIMILARITY = 0.35;  // lower = stricter key; higher = remove more green
  const SMOOTHNESS = 0.12;  // softness at edges

  // ---------- DOM ----------
  const container = document.getElementById('container');
  const camVideo = document.getElementById('camVideo');
  const overlayVideo = document.getElementById('overlayVideo');
  const qrDebug = document.getElementById('qrDebug');
  const message = document.getElementById('message');
  const startBtn = document.getElementById('btnStart');

  // Replace with your video filename
  overlayVideo.src = 'greenscreen.mp4'; // put your greenscreen file in same folder

  // detection canvas (off-screen)
  const detectCanvas = document.createElement('canvas');
  const detectCtx = detectCanvas.getContext('2d');

  // ---------- Three.js setup ----------
  let renderer, scene, camera, planeMesh, videoTexture;
  let lastDetect = 0;
  let qrFound = null;
  let renderSize = new THREE.Vector2();

  function initThree() {
    renderer = new THREE.WebGLRenderer({ alpha:true, antialias:true });
    renderer.setPixelRatio(window.devicePixelRatio || 1);
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.domElement.style.position = 'absolute';
    renderer.domElement.style.top = '0';
    renderer.domElement.style.left = '0';
    container.appendChild(renderer.domElement);

    scene = new THREE.Scene();
    // camera fov approximated for phone; you can tune later
    camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.01, 1000);
    camera.position.set(0,0,0);

    // video texture from overlay video (greenscreen person)
    videoTexture = new THREE.VideoTexture(overlayVideo);
    videoTexture.minFilter = THREE.LinearFilter;
    videoTexture.magFilter = THREE.LinearFilter;
    videoTexture.format = THREE.RGBAFormat;

    // custom chroma-key shader
    const vertexShader = `
      varying vec2 vUv;
      void main() {
        vUv = uv;
        gl_Position = projectionMatrix * modelViewMatrix * vec4(position,1.0);
      }
    `;

    const fragmentShader = `
      uniform sampler2D videoTex;
      uniform vec3 keyColor;
      uniform float similarity;
      uniform float smoothness;
      varying vec2 vUv;

      // convert rgb to luma (used for color distance)
      float lum(vec3 c) {
        return dot(c, vec3(0.299, 0.587, 0.114));
      }

      void main() {
        vec4 c = texture2D(videoTex, vUv);
        // compute color distance (Euclidean)
        float d = distance(c.rgb, keyColor);
        // create alpha via smoothstep
        float a = smoothstep(similarity, similarity + smoothness, d);
        // if mostly keyed (small d), discard pixel to be fully transparent
        if (a < 0.01) discard;

        gl_FragColor = vec4(c.rgb, a);
      }
    `;

    const mat = new THREE.ShaderMaterial({
      uniforms: {
        videoTex: { value: videoTexture },
        keyColor: { value: new THREE.Color(KEY_COLOR.r, KEY_COLOR.g, KEY_COLOR.b) },
        similarity: { value: SIMILARITY },
        smoothness: { value: SMOOTHNESS }
      },
      vertexShader,
      fragmentShader,
      transparent: true,
      depthTest: true,
      side: THREE.DoubleSide
    });

    const geometry = new THREE.PlaneGeometry(1,1,1,1);
    planeMesh = new THREE.Mesh(geometry, mat);
    planeMesh.visible = false; // until QR found
    scene.add(planeMesh);

    window.addEventListener('resize', onWindowResize);
    onWindowResize();
    animate();
  }

  function onWindowResize(){
    const w = window.innerWidth, h = window.innerHeight;
    renderer.setSize(w,h);
    camera.aspect = w/h;
    camera.updateProjectionMatrix();
    renderer.getSize(renderSize);
  }

  // ---------- QR detection & mapping to 3D ----------
  function startQRCodeLoop(){
    // set detect canvas size proportional to camera video
    const vw = camVideo.videoWidth, vh = camVideo.videoHeight;
    const scale = DETECT_WIDTH / vw;
    detectCanvas.width = DETECT_WIDTH;
    detectCanvas.height = Math.round(vh * scale);

    qrDebug.width = detectCanvas.width;
    qrDebug.height = detectCanvas.height;

    setInterval(() => {
      if (camVideo.readyState < 2) return;
      detectCtx.drawImage(camVideo, 0, 0, detectCanvas.width, detectCanvas.height);
      const img = detectCtx.getImageData(0,0,detectCanvas.width, detectCanvas.height);
      const code = jsQR(img.data, img.width, img.height, { inversionAttempts: "dontInvert" });
      // draw debug
      const dctx = qrDebug.getContext('2d');
      dctx.putImageData(img,0,0);
      dctx.strokeStyle = 'lime';
      dctx.lineWidth = 2;
      dctx.beginPath();

      if (code) {
        // code.location has corners in detectCanvas coordinates
        const corners = {
          topLeft: code.location.topLeftCorner,
          topRight: code.location.topRightCorner,
          bottomLeft: code.location.bottomLeftCorner,
          bottomRight: code.location.bottomRightCorner
        };
        drawCornerBox(dctx, corners);
        qrFound = mapCornersToScreen(corners);
        planeMesh.visible = true;
        // apply transform
        applyTransformFromScreenCorners(qrFound);
      } else {
        // not found
        // optionally hide or keep last known
        // planeMesh.visible = false;
      }
    }, QR_SCAN_INTERVAL);
  }

  function drawCornerBox(dctx, corners) {
    dctx.beginPath();
    dctx.moveTo(corners.topLeft.x, corners.topLeft.y);
    dctx.lineTo(corners.topRight.x, corners.topRight.y);
    dctx.lineTo(corners.bottomRight.x, corners.bottomRight.y);
    dctx.lineTo(corners.bottomLeft.x, corners.bottomLeft.y);
    dctx.closePath();
    dctx.stroke();
  }

  // Map corners from detectCanvas → screen pixel coords (renderer dom size)
  function mapCornersToScreen(corners){
    const rw = renderSize.x, rh = renderSize.y;
    const sx = rw / detectCanvas.width;
    const sy = rh / detectCanvas.height;

    return {
      topLeft: { x: corners.topLeft.x * sx, y: corners.topLeft.y * sy },
      topRight: { x: corners.topRight.x * sx, y: corners.topRight.y * sy },
      bottomLeft: { x: corners.bottomLeft.x * sx, y: corners.bottomLeft.y * sy },
      bottomRight: { x: corners.bottomRight.x * sx, y: corners.bottomRight.y * sy }
    };
  }

  // Convert screen corners into world coords, then set plane position + rotation + scale
  function applyTransformFromScreenCorners(c) {
    // helper: screen (px) -> NDC -> unproject to world at chosen z
    const pTL = screenToWorld(c.topLeft);
    const pTR = screenToWorld(c.topRight);
    const pBL = screenToWorld(c.bottomLeft);
    const pBR = screenToWorld(c.bottomRight);

    // compute basis: right and up in world units
    const right = new THREE.Vector3().subVectors(pTR, pTL).normalize();
    const up = new THREE.Vector3().subVectors(pBL, pTL).normalize();
    const normal = new THREE.Vector3().crossVectors(right, up).normalize();

    // make rotation matrix from basis
    const m = new THREE.Matrix4();
    m.makeBasis(right, up, normal);
    planeMesh.quaternion.setFromRotationMatrix(m);

    // center
    const center = new THREE.Vector3().addVectors(pTL, pTR).add(pBL).add(pBR).multiplyScalar(0.25);
    planeMesh.position.copy(center);

    // scale plane to match width and height (plane geometry is 1x1)
    const width = pTR.distanceTo(pTL);
    const height = pBL.distanceTo(pTL);
    planeMesh.scale.set(width, height, 1);
  }

  // screen point (pixel coords relative to renderer dom) -> world point
  function screenToWorld(screenPt) {
    // get NDC
    const rw = renderSize.x, rh = renderSize.y;
    const ndcX = (screenPt.x / rw) * 2 - 1;
    const ndcY = - (screenPt.y / rh) * 2 + 1;
    // choose a depth in NDC [-1..1] -- 0.5 is halfway into camera frustum (tweak if needed)
    const ndc = new THREE.Vector3(ndcX, ndcY, 0.5);
    ndc.unproject(camera);
    return ndc;
  }

  // ---------- camera setup ----------
  async function startCamera() {
    message.textContent = 'Requesting camera...';
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
      camVideo.srcObject = stream;
      await camVideo.play();
      message.textContent = 'Camera started';
      // wait until video metadata ready to size things
      await new Promise(res => {
        if (camVideo.readyState >= 2) res();
        else camVideo.onloadedmetadata = () => res();
      });
      initThree();
      startQRCodeLoop();
      // start overlay video (greenscreen)
      overlayVideo.play().catch(e => {
        // autoplay policies may block audio; video muted should allow
        console.warn('Overlay video play blocked:', e);
      });
      message.style.display = 'none';
    } catch (err) {
      console.error('Camera error', err);
      message.textContent = 'Camera permission denied or not available.';
    }
  }

  // ---------- render loop ----------
  function animate() {
    requestAnimationFrame(animate);
    if (videoTexture) videoTexture.needsUpdate = true;
    renderer.render(scene, camera);
  }

  startBtn.onclick = () => {
    startBtn.disabled = true;
    startCamera();
  };

  // small helper: allow page to auto start on some devices (if user already tapped)
  // but we keep explicit Start button for reliability
  </script>
</body>
</html>
